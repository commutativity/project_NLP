{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3bb1aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:32.598019Z",
     "start_time": "2022-08-18T09:39:32.585988Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41009a",
   "metadata": {},
   "source": [
    "## Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c1b8285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:33.061336Z",
     "start_time": "2022-08-18T09:39:33.012187Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Daten/data_complete.json', 'r', encoding='utf-8') as data:\n",
    "    desc = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "887d85e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:33.186272Z",
     "start_time": "2022-08-18T09:39:33.168895Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Adara Acquisition Corp.', 'ticker': 'ADRA', 'sector': 'Financial Services', 'description': 'Adara Acquisition Corp. does not have significant operations. The company intends to effect a merger, capital stock exchange, asset acquisition, stock purchase, reorganization, or similar business combination with one or more businesses. It focuses on searching for businesses in the consumer products industry and related sectors, including those consumer industry businesses in the health and wellness, e-commerce, discretionary spending, and information technology sectors and related channels of distribution. The company was incorporated in 2020 and is based in Charlotte, North Carolina.'}\n"
     ]
    }
   ],
   "source": [
    "for i in desc:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337389d",
   "metadata": {},
   "source": [
    "## Check for some Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e17d1",
   "metadata": {},
   "source": [
    "Check some metrics and data to delete entries of sectors which do not have many examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36d94d",
   "metadata": {},
   "source": [
    "### Verteilung der Sektoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "940c7cc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:33.919826Z",
     "start_time": "2022-08-18T09:39:33.912819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Financial Services': 2503, 'Healthcare': 1452, 'Communication Services': 403, 'Consumer Defensive': 410, 'Basic Materials': 594, 'Industrials': 1307, 'Consumer Cyclical': 1023, 'Energy': 338, 'Real Estate': 489, 'Technology': 1139, 'Utilities': 164, '': 23, 'Services': 3, 'Industrial Goods': 4, 'Financial': 3, 'Consumer Goods': 1}\n"
     ]
    }
   ],
   "source": [
    "sectoren = {}\n",
    "for comp in desc:\n",
    "    #print(comp)\n",
    "    #print(comp['sector'])\n",
    "    if comp['sector'] in sectoren:\n",
    "        sectoren[comp['sector']] += 1\n",
    "    else:\n",
    "        sectoren[comp['sector']] = 1\n",
    "    #break\n",
    "print(sectoren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1188bba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:35.685844Z",
     "start_time": "2022-08-18T09:39:35.667044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9856"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f401047",
   "metadata": {},
   "source": [
    "#### Bereinigen von zu kleinen Datenmengen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc257798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:36.353821Z",
     "start_time": "2022-08-18T09:39:36.342818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Services', 'Industrial Goods', 'Financial', 'Consumer Goods']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_del = []\n",
    "for val in sectoren.keys():\n",
    "    if sectoren[val] < 100:\n",
    "        to_del.append(val)\n",
    "to_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e51ef37d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:36.810136Z",
     "start_time": "2022-08-18T09:39:36.791174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    c = 0\n",
    "    for i, comp in enumerate(desc):\n",
    "        if comp['sector'] in to_del:\n",
    "            c += 1\n",
    "            del desc[i]\n",
    "    print(c)\n",
    "    if c == 0:\n",
    "        break\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85b253f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:37.837326Z",
     "start_time": "2022-08-18T09:39:37.827502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9822"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29772aed",
   "metadata": {},
   "source": [
    "### Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "877f8548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:49.004213Z",
     "start_time": "2022-08-18T09:39:48.988185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Financial Services': 2503, 'Healthcare': 1452, 'Communication Services': 403, 'Consumer Defensive': 410, 'Basic Materials': 594, 'Industrials': 1307, 'Consumer Cyclical': 1023, 'Energy': 338, 'Real Estate': 489, 'Technology': 1139, 'Utilities': 164} \n",
      " 11\n"
     ]
    }
   ],
   "source": [
    "sectoren = {}\n",
    "for comp in desc:\n",
    "    #print(comp)\n",
    "    #print(comp['sector'])\n",
    "    if comp['sector'] in sectoren:\n",
    "        sectoren[comp['sector']] += 1\n",
    "    else:\n",
    "        sectoren[comp['sector']] = 1\n",
    "    #break\n",
    "print(sectoren, '\\n', len(sectoren))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22291dbb",
   "metadata": {},
   "source": [
    "## Lemmatisierung und Stopwörter entfernen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12ef43",
   "metadata": {},
   "source": [
    "Lemmatize the words to get a more uniform database. Delete stopwords from the database by using the stopword list from the spacy library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e74d407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:53.469419Z",
     "start_time": "2022-08-18T09:39:52.767351Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "all_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96372788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:53.484420Z",
     "start_time": "2022-08-18T09:39:53.471415Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27cf530c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:39:53.500415Z",
     "start_time": "2022-08-18T09:39:53.485415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d759dc1",
   "metadata": {},
   "source": [
    "Lemmatisierung alle Wörter. Außerdem werden alle Stopwörter sowie Zahlen und Interpunktionen entfernt. Diese Liste wird dem jeweiligen Unternehmen in der JSON Datei angehängt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ad34b",
   "metadata": {},
   "source": [
    "### Rough lemmatization and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb91428e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:43:26.070529Z",
     "start_time": "2022-08-18T09:39:57.741433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate(desc):\n",
    "    doc = comp['description']\n",
    "    # Löschen des eigenen Firmennamen aus der Beschreibung\n",
    "    doc = nlp(doc)\n",
    "    #print(doc)\n",
    "    clean_desc = []\n",
    "    # Bestimmte Token werden nicht gespeichert\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM' or token.pos_ == 'PUNCT':\n",
    "            continue\n",
    "        if token.lemma_.lower() in all_stopwords:\n",
    "            continue\n",
    "        #print(token, token.pos_)\n",
    "        clean_desc.append(token.lemma_)\n",
    "    #print(clean_desc)\n",
    "    desc[i]['rough_lemmatization'] = ' '.join(clean_desc)\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0266b5",
   "metadata": {},
   "source": [
    "### Explicit lemmatization and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f553950",
   "metadata": {},
   "source": [
    "#### Find abbreviations in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ec9a4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:46:46.998281Z",
     "start_time": "2022-08-18T09:43:26.073318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n"
     ]
    }
   ],
   "source": [
    "abbreviations = set()\n",
    "for i, comp in enumerate(desc):\n",
    "    doc = comp['description']\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        if token.text[-1] == '.' and token.pos_ != 'PUNCT':\n",
    "            abbreviations.add(token.text)\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b524f0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:46:47.013434Z",
     "start_time": "2022-08-18T09:46:47.000269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '2A.',\n",
       " '2K.',\n",
       " '5G.',\n",
       " 'A.',\n",
       " 'A.C.I.',\n",
       " 'A.G.',\n",
       " 'A.H.',\n",
       " 'A.P.',\n",
       " 'A.S.',\n",
       " 'AEL&P.',\n",
       " 'A\\\\T.',\n",
       " 'B.',\n",
       " 'B.E.',\n",
       " 'B.H.N.',\n",
       " 'B.O.S.',\n",
       " 'B.R.',\n",
       " 'B.R.A.I.N.',\n",
       " 'B.V.',\n",
       " 'BBB\\x96.',\n",
       " 'BasX.',\n",
       " 'Bros.',\n",
       " 'C.',\n",
       " 'C.A.T.',\n",
       " 'C.C.',\n",
       " 'C.E.',\n",
       " 'C.H.',\n",
       " 'C.T.I.',\n",
       " 'C.V.',\n",
       " 'Co.',\n",
       " 'Corp.',\n",
       " 'D.',\n",
       " 'D.C.',\n",
       " 'D.R.',\n",
       " 'Dr.',\n",
       " 'E.',\n",
       " 'E.E.',\n",
       " 'E.W.',\n",
       " 'F.',\n",
       " 'F.A.M.E.',\n",
       " 'F.C.C.',\n",
       " 'F.N.B.',\n",
       " 'F.S.B.',\n",
       " 'F.T.',\n",
       " 'Feb.',\n",
       " 'G.',\n",
       " 'G.A.R.P.',\n",
       " 'G.H.',\n",
       " 'GmbH.',\n",
       " 'H.',\n",
       " 'H.B.',\n",
       " 'H.D.',\n",
       " 'H.I.G.',\n",
       " 'H.I.S.',\n",
       " 'H.K.',\n",
       " 'H.U.',\n",
       " 'I.',\n",
       " 'I.D.',\n",
       " 'I.Q.',\n",
       " 'I.V.',\n",
       " 'Ia.',\n",
       " 'Inc.',\n",
       " 'Ind.',\n",
       " 'IoT.',\n",
       " 'IonQ.',\n",
       " 'J.',\n",
       " 'J.A.',\n",
       " 'J.A.B.',\n",
       " 'J.B.',\n",
       " 'J.L.',\n",
       " 'J.P.',\n",
       " 'J.W.',\n",
       " 'Jr.',\n",
       " 'K.',\n",
       " 'K.K.',\n",
       " 'KGaA.',\n",
       " 'L.',\n",
       " 'L.A.',\n",
       " 'L.B.',\n",
       " 'L.L.C.',\n",
       " 'L.P.',\n",
       " 'L.S.',\n",
       " 'Ltd.',\n",
       " 'M.',\n",
       " 'M.D.',\n",
       " 'M.D.C.',\n",
       " 'M.H.C.',\n",
       " 'M.P.',\n",
       " 'Mr.',\n",
       " 'Mrs.',\n",
       " 'Mt.',\n",
       " 'N.A.',\n",
       " 'N.T.',\n",
       " 'N.V.',\n",
       " 'N.Y.',\n",
       " 'Nov.',\n",
       " 'O.',\n",
       " 'O.S.K.',\n",
       " 'O.W.N.',\n",
       " 'P.',\n",
       " 'P.A.M.',\n",
       " 'P.L.',\n",
       " 'P.L.C.',\n",
       " 'P.S.',\n",
       " 'Q.E.D.',\n",
       " 'R.',\n",
       " 'R.E.A.',\n",
       " 'R.H.D.',\n",
       " 'R.P.',\n",
       " 'R4G.',\n",
       " 'REIT.\\x94.',\n",
       " 'Rev.',\n",
       " 'S.',\n",
       " 'S.A.',\n",
       " 'S.A.A.',\n",
       " 'S.A.B.',\n",
       " 'S.A.P.I.',\n",
       " 'S.A.Q.',\n",
       " 'S.A.R.',\n",
       " 'S.A.S.',\n",
       " 'S.A.U.',\n",
       " 'S.C.',\n",
       " 'S.E.',\n",
       " 'S.E.T.',\n",
       " 'S.L.',\n",
       " 'S.L.U.',\n",
       " 'S.M.E.',\n",
       " 'S.O.D.A.',\n",
       " 'S.P.A.',\n",
       " 'S.R.L.',\n",
       " 'S.S.B.',\n",
       " 'S.T.',\n",
       " 'St.',\n",
       " 'T.',\n",
       " 'T.J.',\n",
       " 'T.K.',\n",
       " 'U.',\n",
       " 'U.K.',\n",
       " 'U.N.',\n",
       " 'U.S.',\n",
       " 'U.S.A.',\n",
       " 'V.',\n",
       " 'V.F.',\n",
       " 'W.',\n",
       " 'W.W.',\n",
       " 'Z.A.G.',\n",
       " 'c.',\n",
       " 'co.',\n",
       " 'd.',\n",
       " 'e.g.',\n",
       " 'eG.',\n",
       " 'http://www.verisresidential.com/.',\n",
       " 'i.',\n",
       " 'i.e.',\n",
       " 'kW.',\n",
       " 'l.',\n",
       " 'm.',\n",
       " 'mbH.',\n",
       " 'onabotulinumtoxinA.',\n",
       " 'vs.',\n",
       " '\\x93RPT\\x94.'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c935e90",
   "metadata": {},
   "source": [
    "Creating a list of abbreviations so these can be deleted. Since there are many unusual abbreviations deleting them is supposed to lead to a cleaner dataset and thereofore a better generalization when classifing the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37ea8b9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:50:10.459208Z",
     "start_time": "2022-08-18T09:46:47.016271Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate(desc):\n",
    "    doc = comp['description']\n",
    "    # Löschen des eigenen Firmennamen aus der Beschreibung\n",
    "    while doc.find(comp['name']) != -1:\n",
    "        doc = doc.replace(comp['name'], '')\n",
    "    doc = nlp(doc)\n",
    "    #print(doc)\n",
    "    clean_desc = []\n",
    "    # Bestimmte Token werden nicht gespeichert\n",
    "    for token in doc:\n",
    "        #print(\"%-20s | %-20s | %-5s | %s\" %\n",
    "        #  (token.text,  token.lemma_, token.pos_, spacy.explain(token.pos_)))\n",
    "        if token.pos_ == 'NUM' or token.pos_ == 'PUNCT' or \\\n",
    "            token.pos_ == 'CCONJ' or len(token) < 3 or \\\n",
    "            token.pos_ == 'SPACE':\n",
    "            continue\n",
    "        if token.text.lower() in all_stopwords:\n",
    "            continue\n",
    "        if token.text in abbreviations:\n",
    "            continue\n",
    "        #print(token, token.pos_)\n",
    "        clean_desc.append(token.lemma_)\n",
    "    #print(clean_desc)\n",
    "    desc[i]['explicit_lemmatization'] = ' '.join(clean_desc)\n",
    "    desc[i]['search_engine'] = ' '.join([token.upper() for token in clean_desc])\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e797096",
   "metadata": {},
   "source": [
    "#### Speichern neuer JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "849471f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:50:11.053140Z",
     "start_time": "2022-08-18T09:50:10.461136Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Daten/Unternehmen_preprocessed.json', 'w') as wf:\n",
    "    json.dump(desc, wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe849a",
   "metadata": {},
   "source": [
    "Unternehmen_rough_lemmatization enthält eine sehr einfache Version von Lemmatisierung. Nachdem einige anderen Analysen mit dem Datensatz durchgeführt wurden, mussten jedoch weitere Begriffe, bzw. Inhalte gefiltert werden. Dazu gehören durch die Tokenisierung enstandene Fehler. Dazu gehören z. B. einzelne Buchstaben aus Krankheiten wie 'Hepatitis B'. Außerdem wurde die Firmenbezeichnung aus der Beschreibung gefiltert. \n",
    "Außerdem wird eine extra \"Datenbank\" erzeugt, um in folgenden Schritten eine Suchmaschine erzeugen zu können.\n",
    "Aufgrund der oftmals ungenauen Ergebnisse wurde auf eine Verarbeitung mittels eines \"Porter Stemmer\" verzichtet.\n",
    "\n",
    "Die Ergebnisse der unterschiedlichen Lemmatisierung bzw. Vorverarbeitung werden abschließend verglichen?????????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515dee9c",
   "metadata": {},
   "source": [
    "## Wordcloud with different sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44483941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T08:36:28.127041Z",
     "start_time": "2022-06-23T08:36:28.107006Z"
    }
   },
   "outputs": [],
   "source": [
    "print(desc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[i] = s[i].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c455c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T08:53:16.806711Z",
     "start_time": "2022-06-23T08:53:16.794711Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_dir = {}\n",
    "for comp in desc:\n",
    "    if comp['sector'] not in comp_dir:\n",
    "        comp_dir[comp['sector']] = []\n",
    "    if len(comp_dir) == 11:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225d8da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T08:53:17.071316Z",
     "start_time": "2022-06-23T08:53:17.042352Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, comp in enumerate(desc):\n",
    "    comp_dir[comp['sector']].append(comp['clean_description'].upper())\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i} Firmenberichte verarbeitet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b22fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T08:53:17.978981Z",
     "start_time": "2022-06-23T08:53:17.967900Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in comp_dir.keys():\n",
    "    lst = comp_dir[key]\n",
    "    lst = ' '.join(lst)\n",
    "    comp_dir[key] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838cfe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T09:04:25.651296Z",
     "start_time": "2022-06-23T09:01:26.896359Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in comp_dir.keys():\n",
    "    print(f'Wordcloud for the company sector {key}')\n",
    "    wordcloud =  WordCloud(background_color=\"white\", width = 5000,\n",
    "                       height = 3500, max_words = 50).generate(comp_dir[key])\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45638895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bb1aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:26:01.728371Z",
     "start_time": "2022-08-27T05:25:53.430011Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41009a",
   "metadata": {},
   "source": [
    "## Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1b8285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:26:01.882750Z",
     "start_time": "2022-08-27T05:26:01.731208Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Daten/data_complete.json', 'r', encoding='utf-8') as data:\n",
    "    desc = json.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337389d",
   "metadata": {},
   "source": [
    "## Check for some Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e17d1",
   "metadata": {},
   "source": [
    "Check some metrics and data to delete entries of sectors which do not have many examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36d94d",
   "metadata": {},
   "source": [
    "### Distribution of sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "940c7cc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:13.996943Z",
     "start_time": "2022-08-27T05:29:13.974772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Financial Services': 2503, 'Healthcare': 1452, 'Communication Services': 403, 'Consumer Defensive': 410, 'Basic Materials': 594, 'Industrials': 1307, 'Consumer Cyclical': 1023, 'Energy': 338, 'Real Estate': 489, 'Technology': 1139, 'Utilities': 164, '': 23, 'Services': 3, 'Industrial Goods': 4, 'Financial': 3, 'Consumer Goods': 1}\n"
     ]
    }
   ],
   "source": [
    "sectoren = {}\n",
    "for comp in desc:\n",
    "    #print(comp)\n",
    "    #print(comp['sector'])\n",
    "    if comp['sector'] in sectoren:\n",
    "        sectoren[comp['sector']] += 1\n",
    "    else:\n",
    "        sectoren[comp['sector']] = 1\n",
    "    #break\n",
    "print(sectoren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1188bba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:14.214059Z",
     "start_time": "2022-08-27T05:29:14.198894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9856"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f401047",
   "metadata": {},
   "source": [
    "#### Bereinigen von zu kleinen Datenmengen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc257798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:15.491283Z",
     "start_time": "2022-08-27T05:29:15.482257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Services', 'Industrial Goods', 'Financial', 'Consumer Goods']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_del = []\n",
    "for val in sectoren.keys():\n",
    "    if sectoren[val] < 100:\n",
    "        to_del.append(val)\n",
    "to_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51ef37d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:16.378562Z",
     "start_time": "2022-08-27T05:29:16.359562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    c = 0\n",
    "    for i, comp in enumerate(desc):\n",
    "        if comp['sector'] in to_del:\n",
    "            c += 1\n",
    "            del desc[i]\n",
    "    #print(c)\n",
    "    if c == 0:\n",
    "        break\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b253f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:17.361874Z",
     "start_time": "2022-08-27T05:29:17.348873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9822"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29772aed",
   "metadata": {},
   "source": [
    "### Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "877f8548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:18.877454Z",
     "start_time": "2022-08-27T05:29:18.855932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Financial Services': 2503, 'Healthcare': 1452, 'Communication Services': 403, 'Consumer Defensive': 410, 'Basic Materials': 594, 'Industrials': 1307, 'Consumer Cyclical': 1023, 'Energy': 338, 'Real Estate': 489, 'Technology': 1139, 'Utilities': 164} \n",
      " 11\n"
     ]
    }
   ],
   "source": [
    "sectoren = {}\n",
    "for comp in desc:\n",
    "    #print(comp)\n",
    "    #print(comp['sector'])\n",
    "    if comp['sector'] in sectoren:\n",
    "        sectoren[comp['sector']] += 1\n",
    "    else:\n",
    "        sectoren[comp['sector']] = 1\n",
    "    #break\n",
    "print(sectoren, '\\n', len(sectoren))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22291dbb",
   "metadata": {},
   "source": [
    "## Lemmatization and stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12ef43",
   "metadata": {},
   "source": [
    "Lemmatize the words to get a more uniform database. Delete stopwords from the database by using the stopword list from the spacy library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e74d407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:47.815090Z",
     "start_time": "2022-08-27T05:29:47.139085Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "all_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96372788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:48.980107Z",
     "start_time": "2022-08-27T05:29:48.962140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cf530c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:29:51.943156Z",
     "start_time": "2022-08-27T05:29:51.935158Z"
    }
   },
   "outputs": [],
   "source": [
    "#all_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d759dc1",
   "metadata": {},
   "source": [
    "Lemmatization of all words. Also, all stop words as well as numbers and punctuation are removed. This list is attached to the respective company in the JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ad34b",
   "metadata": {},
   "source": [
    "### Rough lemmatization and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb91428e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:33:24.623956Z",
     "start_time": "2022-08-27T05:30:03.250970Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, comp in enumerate(desc):\n",
    "    doc = comp['description']\n",
    "    # LÃ¶schen des eigenen Firmennamen aus der Beschreibung\n",
    "    doc = nlp(doc)\n",
    "    #print(doc)\n",
    "    clean_desc = []\n",
    "    # Bestimmte Token werden nicht gespeichert\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM' or token.pos_ == 'PUNCT':\n",
    "            continue\n",
    "        if token.lemma_.lower() in all_stopwords:\n",
    "            continue\n",
    "        #print(token, token.pos_)\n",
    "        clean_desc.append(token.lemma_)\n",
    "    #print(clean_desc)\n",
    "    desc[i]['rough_lemmatization'] = ' '.join(clean_desc)\n",
    "    #if i % 500 == 0:\n",
    "        #print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0266b5",
   "metadata": {},
   "source": [
    "### Explicit lemmatization and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f553950",
   "metadata": {},
   "source": [
    "#### Find abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ec9a4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:36:42.009173Z",
     "start_time": "2022-08-27T05:33:24.625945Z"
    }
   },
   "outputs": [],
   "source": [
    "abbreviations = set()\n",
    "for i, comp in enumerate(desc):\n",
    "    doc = comp['description']\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        if token.text[-1] == '.' and token.pos_ != 'PUNCT':\n",
    "            abbreviations.add(token.text)\n",
    "    #if i % 500 == 0:\n",
    "    #    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b524f0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:38:56.450012Z",
     "start_time": "2022-08-27T05:38:56.440014Z"
    }
   },
   "outputs": [],
   "source": [
    "#abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c935e90",
   "metadata": {},
   "source": [
    "Creating a list of abbreviations so these can be deleted. Since there are many unusual abbreviations deleting them is supposed to lead to a cleaner dataset and thereofore a better generalization when classifing the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37ea8b9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:42:23.920317Z",
     "start_time": "2022-08-27T05:39:06.836465Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, comp in enumerate(desc):\n",
    "    doc = comp['description']\n",
    "    # LÃ¶schen des eigenen Firmennamen aus der Beschreibung\n",
    "    while doc.find(comp['name']) != -1:\n",
    "        doc = doc.replace(comp['name'], '')\n",
    "    doc = nlp(doc)\n",
    "    #print(doc)\n",
    "    clean_desc = []\n",
    "    # Bestimmte Token werden nicht gespeichert\n",
    "    for token in doc:\n",
    "        #print(\"%-20s | %-20s | %-5s | %s\" %\n",
    "        #  (token.text,  token.lemma_, token.pos_, spacy.explain(token.pos_)))\n",
    "        if token.pos_ == 'NUM' or token.pos_ == 'PUNCT' or \\\n",
    "            token.pos_ == 'CCONJ' or len(token) < 3 or \\\n",
    "            token.pos_ == 'SPACE':\n",
    "            continue\n",
    "        if token.text.lower() in all_stopwords:\n",
    "            continue\n",
    "        if token.text in abbreviations:\n",
    "            continue\n",
    "        #print(token, token.pos_)\n",
    "        clean_desc.append(token.lemma_)\n",
    "    #print(clean_desc)\n",
    "    desc[i]['explicit_lemmatization'] = ' '.join(clean_desc)\n",
    "    desc[i]['search_engine'] = ' '.join([token.upper() for token in clean_desc])\n",
    "    #if i % 500 == 0:\n",
    "    #    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e797096",
   "metadata": {},
   "source": [
    "#### Save new json with additional preprocesses data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "849471f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T05:42:24.504439Z",
     "start_time": "2022-08-27T05:42:23.922258Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Daten/Unternehmen_preprocessed.json', 'w') as wf:\n",
    "    json.dump(desc, wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe849a",
   "metadata": {},
   "source": [
    "Unternehmen_rough_lemmatization enthÃ¤lt eine sehr einfache Version von Lemmatisierung. Nachdem einige anderen Analysen mit dem Datensatz durchgefÃ¼hrt wurden, mussten jedoch weitere Begriffe, bzw. Inhalte gefiltert werden. Dazu gehÃ¶ren durch die Tokenisierung enstandene Fehler. Dazu gehÃ¶ren z. B. einzelne Buchstaben aus Krankheiten wie 'Hepatitis B'. AuÃerdem wurde die Firmenbezeichnung aus der Beschreibung gefiltert. \n",
    "AuÃerdem wird eine extra \"Datenbank\" erzeugt, um in folgenden Schritten eine Suchmaschine erzeugen zu kÃ¶nnen.\n",
    "Aufgrund der oftmals ungenauen Ergebnisse wurde auf eine Verarbeitung mittels eines \"Porter Stemmer\" verzichtet.\n",
    "\n",
    "Die Ergebnisse der unterschiedlichen Lemmatisierung bzw. Vorverarbeitung werden abschlieÃend verglichen?????????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
